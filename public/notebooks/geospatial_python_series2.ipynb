{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCC - UChicago, 2025\n",
    "\n",
    "## Geospatial Python Part 2: Scaling Geospatial Workflows with Dask\n",
    "\n",
    "### Instructors:\n",
    "\n",
    "- Hamid Dashti (hdashti@uchicago.edu)\n",
    "- Parmanand Sinha (pnsinha@uchicago.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands\n",
    "\n",
    "- `Shift` + `Enter`: Run and move to the next cell.\n",
    "- `Ctrl` + `Enter`: Run the cell in place.\n",
    "- `Alt` + `Enter`: Run and insert a new cell below.\n",
    "- To delete a cell: Press `esc` to enter command mode, then press `cmd`+`m`+`d`.\n",
    "- To insert a new cell below: Press `esc` then `b`.\n",
    "- To insert a new cell above: Press `esc` then `a`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "- **Introduction to xarray:**\n",
    "  - Data exploration\n",
    "  - Indexing\n",
    "- **Visualizing data with xarray:**\n",
    "\n",
    "  - Static plots\n",
    "  - Interactive plotting\n",
    "\n",
    "- **Computation with Xarray:**\n",
    "\n",
    "  - Built-in functions\n",
    "  - Costume functions\n",
    "\n",
    "  **End with and exercise**\n",
    "\n",
    "**Second Half:**\n",
    "\n",
    "- **Scaling computations with Dask:**\n",
    "  - Handling out-of-memory (large) datasets\n",
    "  - Parallel programming\n",
    "- **Accessing Cloud-Based Data Catalogs:**\n",
    "  - Searching Earth Engine and Planetary Computer data (explore a STAC Catalog)\n",
    "  - Integrating cloud data into xarray workflows\n",
    "\n",
    "If time allows:\n",
    "\n",
    "- **Running Dask on HPC/HTC (Requires CHTC accounts):**\n",
    "  - Setting up Dask on UW HPC using vscode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Geospatial Data Formats for Climate and Satellite Data**\n",
    "\n",
    "## Key Formats\n",
    "\n",
    "Common geospatial file formats frequently used for climate and satellite data:\n",
    "\n",
    "### NetCDF (Network Common Data Form; <u>Our focus today</u>)\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Storing and sharing multidimensional scientific array-based data with comprehensive metadata.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Self-describing with rich metadata for efficient algorithm development.\n",
    "- Scalable for efficient access to subsets of large datasets, even remotely.\n",
    "- Appendable for data addition without structure redefinition.\n",
    "- Sharable with support for multi-user access.\n",
    "\n",
    "**Common Applications:**\n",
    "\n",
    "- Gridded climate data\n",
    "- Satellite images\n",
    "- Earth system model outputs\n",
    "\n",
    "**CF Conventions (Climate and Forecast):**\n",
    "\n",
    "- Standardized metadata for self-description and interoperability.\n",
    "- Ensures variables have associated descriptions, physical units, and spatiotemporal coordinates.\n",
    "- Enables software tools to work effectively with minimal user intervention.\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [NetCDF Website](https://www.unidata.ucar.edu/software/netcdf/)\n",
    "- [CF Conventions](http://cfconventions.org/)\n",
    "\n",
    "### HDF5 (Hierarchical Data Format version 5)\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Storing complex heterogeneous datasets.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Hierarchical data organization with groups and datasets.\n",
    "- Self-describing with metadata within the file.\n",
    "- Multiple data type support (integers, floats, strings, complex numbers).\n",
    "- Chunking and compression for efficient storage and access.\n",
    "- Large file and dataset support (terabytes to petabytes).\n",
    "- Parallel processing capabilities.\n",
    "\n",
    "**Common Applications:**\n",
    "\n",
    "- Many satellite data (e.g. MODIS) is HDF5.\n",
    "- Earth system model outputs.\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [HDF5 Website](https://www.hdfgroup.org/solutions/hdf5/)\n",
    "\n",
    "### Zarr\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Efficient parallel processing and cloud storage of large datasets.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Stores data in chunks across multiple files. Makes reading and writing large datasets faster.\n",
    "- Optimized for high-performance computing (HPC), high-throughput computing (HTC), and cloud environments.\n",
    "\n",
    "**Common Applications:**\n",
    "\n",
    "- Large-scale scientific datasets\n",
    "- Cloud-based data analysis\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [Zarr Website](https://zarr.dev/)\n",
    "\n",
    "### STAC (SpatioTemporal Asset Catalog)\n",
    "\n",
    "**Purpose:**:\n",
    "\n",
    "- Simplify search and discovery of geospatial data across different providers and platforms.\n",
    "- Enable interoperability between various tools and applications working with geospatial data.\n",
    "- Facilitate easier cloud storage and access for large datasets.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Uses JSON files to describe assets, providing information like location, time, data type, metadata, and availability.\n",
    "- Flexible and extensible, allowing customization for specific data types and needs.\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [STAC Website](https://stacspec.org/en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Data and Speed: The Next Challenge\n",
    "\n",
    "We've covered the basics of climate data processing in Python. Now, let's tackle the real challenges:\n",
    "\n",
    "1. **Out-of-memory data:** What happens when datasets are too large to fit in memory?\n",
    "2. **Speeding up computations:** How can we analyze data faster, especially with large datasets?\n",
    "\n",
    "In the next session, we'll introduce `Dask`, a powerful tool for handling these issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Dask?** - Dask is an open-source library designed to provide parallelism to the existing Python stack. It provides integrations with Python libraries like NumPy, Pandas, **Xarray** and scikit-learn to enable parallel execution across multiple cores, processors, and computers without having to learn new libraries or languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling with Dask\n",
    "import xarray as xr\n",
    "import dask\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Get the number of cores\n",
    "n_cores = 2\n",
    "# Specify the number of threads per worker\n",
    "threads_per_worker = 2  # adjust this based on your workload\n",
    "\n",
    "client = Client(n_workers=n_cores, threads_per_worker=threads_per_worker)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple example of a 2D array of numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1000, 4000)\n",
    "ones_np = np.ones(shape)\n",
    "print(\"Size:\", ones_np.nbytes / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an equivalent Dask array using dask.array.ones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_da = dask.array.ones(shape)\n",
    "ones_da\n",
    "\n",
    "# Note: 1 MiB = 1,048,576 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have not \"chunk\" the data yet. There is only one chunk with same shape as our Array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets chunk the data into 1000 by 1000 blocks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_size = (1000, 1000)\n",
    "ones_da = dask.array.ones(shape, chunks=chunks_size)\n",
    "ones_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pay close attention to the chunks size and the memory usage.\n",
    "\n",
    "### Dask graph\n",
    "A Dask graph, often referred to as a Dask task graph or computation graph, represents the logical structure of a computation. In this graph, each node represents a task or operation, and the edges represent dependencies between these tasks.\n",
    "\n",
    "**Thus, note that Dask does not implement any computations until you explicitly request them. At this stage, Dask only graphs the \"workflow\" of all tasks you've asked it to perform for you.**\n",
    "\n",
    "You can utilize the dask.array.visualize() function to visualize the Dask graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you get error related to graphviz when plotting\n",
    "#!pip uninstall graphviz\n",
    "# conda install -c conda-forge graphviz\n",
    "# conda install -c conda-forge python-graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dask graph\n",
    "dask.visualize(ones_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to numpy, we can perform arithmetic operations on dask arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is lazy, it graphs the tasks but not doing it, until we specifically ask for it through compute\n",
    "ones_mean = ones_da.mean()\n",
    "ones_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key distinction between Dask and NumPy lies in Dask's \"lazy\" evaluation approach, where computations are deferred until explicitly requested.\n",
    "\n",
    "The provided code represents merely the computation graph, outlining the sequence of operations, rather than executing the computations themselves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(ones_da.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the mean, we can use the .compute() method to trigger the computation and get the result as a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate\n",
    "ones_mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize the calculation\n",
    "\n",
    "Now we know about chunking, what about parallel computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is designed to take precisely 4 seconds when executed sequentially:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def inc(x):\n",
    "    # Takes two seconds to compute\n",
    "    time.sleep(2)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def dec(y):\n",
    "    # Takes one second to compute\n",
    "    time.sleep(1)\n",
    "    return y - 1\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    # Takes one seconds to compute\n",
    "    time.sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with dask.delayed we can make the above computation lazy. Meaning we only design the computation graph but not doing the computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = dask.delayed(inc)\n",
    "dec = dask.delayed(dec)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how fast the cell runs! This is because the dask.delayed calls are building up a task graph, but not actually executing it.\n",
    "\n",
    "Lets visualize the computation graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dask graph for calculation of z\n",
    "z.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your guess for the time it will take to compute z?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask-Xarray for Large-Scale Gridded Geospatial Data Analysis\n",
    "\n",
    "You've already explored the individual strengths of Xarray and Dask - the former providing a familiar and intuitive interface for labeled arrays, and the latter unlocking parallel processing on massive datasets. Now, let's delve into the exciting stuff when they're combined:\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "- **Parallel Processing:** Dask distributes data and computations across multiple cores or worker processes, enabling significantly faster analysis on large datasets. Operations like aggregation, reshaping, and arithmetic leverage distributed computing power, drastically reducing analysis times.\n",
    "- **Lazy Evaluation:** Dask-Xarray adopts a \"lazy\" approach, deferring actual computations until absolutely necessary.\n",
    "  - This optimizes resource utilization by focusing only on the required parts of complex workflows, further boosting efficiency.\n",
    "  - Reproducibility of the research. Computation the metadata than the data is more transferable, especially if the metadata is the same across datasets (e.g. STAC)\n",
    "- **Streaming:** For datasets exceeding disk capacity, Dask-Xarray employs streaming to process data in chunks, eliminating the need to load everything at once.\n",
    "- **Familiar API:** Xarray's intuitive API, reminiscent of NumPy and pandas, ensures a consistent experience regardless of data location: in-memory arrays or out-of-memory Dask arrays. This minimizes the learning curve and simplifies code adaptation.\n",
    "- **Flexibility:** Dask-Xarray adapts to your hardware and software environment, whether you're working on a single workstation, multi-core machine, cluster, or cloud platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! now we know how to use Dask to scale our computation. Let's go back to our air temprature dataset and see how we can use Dask to scale our computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just open the DataArrays it will be loaded into memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"./data/air.mon.mean.nc\")\n",
    "da = xr.open_dataarray(\"./data/air.mon.mean.nc\")\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data is huge then this approach fails! so we need to use `chunks` (remember from dask) argument to load the data in chunks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the air temprature Dataset with Dask enabled\n",
    "da = xr.open_dataarray(\n",
    "    \"./data/air.mon.mean.nc\",\n",
    "    chunks={\n",
    "        \"time\": 100,\n",
    "        \"lat\": \"auto\",\n",
    "        \"lon\": \"auto\",\n",
    "    },\n",
    ")\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data has been loaded as a Dask array, with the chunks argument determining how the data is partitioned into manageable chunks.\n",
    "\n",
    "- This implies that the entire dataset is not loaded into memory; instead, a computational graph is constructed.\n",
    "- This graph consists of tasks that are executed on-demand, following a \"lazy\" evaluation strategy similar to the example demonstrated at the beginning of the session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from the dataset there are two ways:\n",
    "\n",
    "1. `.data` which returns a dask array\n",
    "2. `.to_numpy` and `.values`, which means it will call the `compute()`\n",
    "\n",
    "- Use `.to_numpy` instead of `.values` as it retruns more generlizable array (e.g. sparse arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is a dask array and lazy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the numpy array use `.to_numpy()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = da.to_numpy()\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't worry about the nans, this is land temperature data and it is expected to have nans over the ocean and other places.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the computation on xarray when calling chunks is lazy.\n",
    "\n",
    "It means that the actual computation is deferred until it is explicitly needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do an example of a simple calculation with Dask-xarray by taking the mean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = da.mean()\n",
    "std = da.std()\n",
    "mean_std = mean + std\n",
    "mean_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note its fast since its a lazy computation ---> no actual computation is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the dask graph for this calculation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_std, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to perform the computation:\n",
    "\n",
    "1. `.compute()`: returns an xarray object. Recommended for smaller datasets where outputs are small enough to fit into memory.\n",
    "2. `.load()`: similar to compute but returns a dask object. (Not generally recommended for out-of-memory situations).\n",
    "3. `.persist():` does the computation but holds the results in the distributed cluster memory. Most common for out-of-memory situations but it requires access to clusters/clouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_calculated = mean_std.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dask is executing the graph and calculating the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the trend but with Dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_annual = da.resample(time=\"Y\").mean()\n",
    "da_annual = da_annual.chunk(dict(time=-1))\n",
    "da_annual = da_annual.chunk({\"lat\": 100, \"lon\": 100, \"time\": -1})\n",
    "da_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the linear trend using numpy polyfit\n",
    "def linear_trend(y):\n",
    "    # y is the variable of interest\n",
    "    # Check if there is any NaN in y\n",
    "    if np.any(np.isnan(y)):\n",
    "        # Return NaN as slope\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Create an array of indices as x\n",
    "        x = np.arange(len(y))\n",
    "        # Return only the slope of the linear fit\n",
    "        return np.polyfit(x, y, 1)[0]\n",
    "\n",
    "\n",
    "trend = xr.apply_ufunc(\n",
    "    linear_trend,\n",
    "    da_annual.variable,\n",
    "    input_core_dims=[[\"time\"]],\n",
    "    output_core_dims=[[]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float],\n",
    ")\n",
    "trend_dataarray = xr.DataArray(\n",
    "    trend, dims=[\"lat\", \"lon\"], coords={\"lat\": ds.lat, \"lon\": ds.lon}\n",
    ")\n",
    "trend_dataarray.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import optimize\n",
    "\n",
    "(optimized,) = optimize(trend.data)\n",
    "optimized.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Dask to parallelize computations, there can be overhead associated with parallelization, chunking, and data movement between workers. In some cases, for smaller datasets, the overhead of parallelization can outweigh the benefits of parallel processing, resulting in longer execution times compared to a non-parallelized approach.\n",
    "\n",
    "In this specific case, with a small array (77 time steps, 360 latitudes, and 720 longitudes), the overhead introduced by Dask's parallelization may dominate the computation time. Dask is designed to handle larger-than-memory datasets efficiently by breaking them into smaller chunks and processing them in parallel. However, for smaller datasets that can fit into memory, the overhead of parallelization may outweigh the benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real out-of-memory-example:\n",
    "\n",
    "**Do not try to visualize! Its too big and will fail**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigshape = (200000, 40000)\n",
    "chunk_shape = (1000, 1000)  # define your chunk shape\n",
    "big_ones = dask.array.ones(bigshape, chunks=chunk_shape)\n",
    "print(\"Size is:\", big_ones.nbytes / 1e9, \"GB! To big to fit in memory\")\n",
    "big_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a fake xarray object and give it fake lat and lon ( in real world this big data could be your geospatial satellite/climate data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ones_xr = xr.DataArray(\n",
    "    big_ones,\n",
    "    dims=[\"lat\", \"lon\"],\n",
    "    coords={\"lat\": np.arange(bigshape[0]), \"lon\": np.arange(bigshape[1])},\n",
    "    name=\"big_ones\",\n",
    "    attrs={\"units\": \"m\"},\n",
    ")\n",
    "big_ones_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do big calculations with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mean = big_ones.mean() + big_ones.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "ProgressBar().register()\n",
    "with ProgressBar():\n",
    "    result = big_mean.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice:\n",
    "\n",
    "- Try to chunk the data along the time dimension and calculate the linear trend using the apply_ufunc function. What do you observe?\n",
    "- Also experiement with different lat and lon chunk sizes and examine the difference in execution time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close Dask client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bonus section:** Microsoft planetary computer uses Xarray and Dask for Gepspatial data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data from Microsoft Planetary Computer\n",
    "\n",
    "The [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/) is a powerful cloud platform specifically designed for researchers in climate and environmental fields. It provides **unprecedented access to a vast repository of data**, including:\n",
    "\n",
    "- **Climate observations**\n",
    "- **Satellite imagery**\n",
    "- **Model outputs**\n",
    "\n",
    "The Planetary Computer leverages the **STAC API (SpatioTemporal Asset Catalog)**, a standardized interface that makes it easy to discover, search, and access datasets based on specific criteria like location, time period, and data type.\n",
    "\n",
    "**Here's a quick overview of the process:**\n",
    "\n",
    "**1. Explore the Data Catalog:** Browse the Planetary Computer's extensive collection of datasets through the user-friendly Data Catalog. Filter by parameters like spatial coverage, temporal resolution, and data type to find the resources relevant to your research.\n",
    "\n",
    "**2. Utilize the STAC API:** Interact with the data programmatically using the STAC API. This protocol enables flexible querying, subsetting, and retrieval of specific data segments you need for your analysis.\n",
    "\n",
    "**3. Download or Process Data:** Download the retrieved data directly to your local machine or leverage cloud-based processing environments within the Planetary Computer platform.\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- **[Official Documentation](https://planetarycomputer.microsoft.com/docs/overview/about)** .\n",
    "- **[Catalog](https://planetarycomputer.microsoft.com/catalog)**\n",
    "- **[Community](https://github.com/microsoft/PlanetaryComputer)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pystac-client planetary-computer odc.stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import planetary_computer\n",
    "import odc.stac\n",
    "import matplotlib.pyplot as plt\n",
    "from pystac.extensions.eo import EOExtension as eo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the data, we’ll create a pystac_client.Client. We’ll explain the modifier part later on, but it’s what lets us download the data assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define area of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_of_interest = [-122.001, 47, -122, 47.001]\n",
    "time_of_interest = \"2021-01-01/2021-12-31\"\n",
    "# area_of_interest = {\"type\": \"Point\", \"coordinates\": [-122.2751, 47.5469]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the catalog for the landsat data collection 2 for the year 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = catalog.search(\n",
    "    collections=[\"landsat-c2-l2\"],\n",
    "    # intersects=area_of_interest,\n",
    "    bbox=bbox_of_interest,\n",
    "    datetime=time_of_interest,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(f\"Returned {len(items)} Items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect one item to see what it looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(items)\n",
    "items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data where each item is a time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "bands_of_interest = [\"nir08\"]\n",
    "data_list = []\n",
    "\n",
    "for item in items:\n",
    "    data = odc.stac.stac_load(\n",
    "        [item], bands=bands_of_interest, bbox=bbox_of_interest\n",
    "    ).isel(time=0)\n",
    "    data_list.append(data)\n",
    "\n",
    "combined_data = xr.concat(data_list, dim=\"item\")\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.nir08.mean([\"x\", \"y\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.nir08.mean(\"item\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nir08\"].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
